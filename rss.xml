<?xml version="1.0" encoding="utf-8"?>






<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>李清华的博客</title>
        <link>https://famousli.cn/</link>
        <description>李清华的博客</description>
        <generator>Hugo 0.113.0 https://gohugo.io/</generator>
        
            <language>zh-CN</language>
        
        
            <managingEditor>famousli2799@gmail.com (李清华)</managingEditor>
        
        
            <webMaster>famousli2799@gmail.com (李清华)</webMaster>
        
        
        <lastBuildDate>Fri, 03 Nov 2023 11:33:26 &#43;0800</lastBuildDate>
        
            <atom:link rel="self" type="application/rss&#43;xml" href="https://famousli.cn/rss.xml" />
        
        
            <item>
                <title>多智能体深度强化学习</title>
                <link>https://famousli.cn/tech/mdrl/</link>
                <guid isPermaLink="true">https://famousli.cn/tech/mdrl/</guid>
                <pubDate>Wed, 21 Jun 2023 10:52:23 &#43;0800</pubDate>
                
                    <author>famousli2799@gmail.com (李清华)</author>
                
                
                
                    <description>&lt;h1 id=&#34;多智能体深度强化学习multi-agent-deep-reinforcement-learningmdrl&#34;&gt;多智能体深度强化学习（Multi-agent Deep Reinforcement Learning，MDRL）&lt;/h1&gt;
&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;
&lt;p&gt;单智能体系统：智能体只需要根据自身的动作和环境的交互即可完成学习的任务。&lt;/p&gt;
&lt;p&gt;多智能体系统：每个智能体不仅需要考虑自身动作对环境造成的影响和获得的回报，还需要观测其他智能体的行为和影响。&lt;/p&gt;
&lt;p&gt;强化学习：通常将待解决的问题描述为 马尔科夫决策过程。&lt;/p&gt;
&lt;p&gt;多智能体深度强化学习：在多智能体系统中，马尔科夫决策过程被拓展为 马尔科夫博弈（又称为随机博弈，Markov/stochastic game）。&lt;/p&gt;
&lt;p&gt;马尔科夫博弈中的纳什均衡（Nash equilibrium）：在多个智能体中达成的一个不动点。对于其中任意一个智能体来说，当其他智能体策略确定时，任一智能体都无法通过采取其他的策略来获得更高的累积回报。也就是说，每个人的策略都是对其他人的策略的最优策略。&lt;/p&gt;
&lt;h2 id=&#34;多智能体的关系模式&#34;&gt;多智能体的关系模式&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;智能体之间为竞争关系&lt;/p&gt;
&lt;p&gt;每个智能体都为得到自己的最大奖励而竞争，每个智能体都有自己的奖励值。智能体的目标是最大化自身的奖励值，同时尽可能最小化对方奖励值。典型的算法有：minimax Q-learning 算法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;智能体之间是半合作半竞争关系&lt;/p&gt;
&lt;p&gt;智能体的奖励函数之间并没有确定的正负关系，目标是要多个智能体共同达到一个均衡点，这类问题通常与博弈论中的纳什均衡概念相关。典型的算法有：Nash Q -learning（纳什Q学习）、Correlated Q -learning（相关Q学习）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;智能体之间为合作关系&lt;/p&gt;
&lt;p&gt;智能体之间为合作关系的时候，所有智能体都在为共同目标努力，因此获得的奖励是相同的。典型的算法有：Team Q-learning（团队Q学习）、Distributed Q -learning（分布式Q学习）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;合作关系的-mdrl-方法&#34;&gt;合作关系的 MDRL 方法&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;无协作学习：智能体之间没有协作机制，每个智能体将其他智能体看作环境的一部分，智能体独立地与环境进行交互并自发地形成行为策略。&lt;/li&gt;
&lt;li&gt;通信学习（显式通信）：通信机制允许每个智能体根据自身的局部观测和从其他智能体接收到的信息动态调整自己的策略，而非简单将其他智能体视为环境的一部分。&lt;/li&gt;
&lt;li&gt;协作学习（隐式通信）：智能体通过一个集中式的 Critic 网络接收所有智能体的局部观测进行训练，而执行时智能体仅依靠自身的观测进行决策。协作学习的实现方法分为两类：基于值函数分解(Value Function Decomposition, VFD)的协作学习和基于中心化值函数（Centralized Value Function, CVF）的协作学习。&lt;/li&gt;
&lt;li&gt;建模学习：智能体在不知道其他智能策略的情况下，通过对其他智能体建模的方式分析并预测其行为。&lt;/li&gt;
&lt;/ol&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://famousli.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</category>
                                
                            
                        
                    
                        
                    
                
            </item>
        
            <item>
                <title>DDPG算法</title>
                <link>https://famousli.cn/tech/drl-ddpg/</link>
                <guid isPermaLink="true">https://famousli.cn/tech/drl-ddpg/</guid>
                <pubDate>Sat, 17 Jun 2023 16:52:04 &#43;0800</pubDate>
                
                    <author>famousli2799@gmail.com (李清华)</author>
                
                
                
                    <description>&lt;h1 id=&#34;ddpg算法&#34;&gt;DDPG算法&lt;/h1&gt;
&lt;p&gt;DDPG算法一共包含了四个网络，分别是：Actor网络，Actor目标网络，Critic网络，Critic目标网络，2个Actor 网络的结构相同，2个Critic网络的结构相同，四个网络的功能如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actor网络：负责策略网络参数 $\theta$ 的迭代更新，负责根据当前状态 S 选择当前动作 A，用于和环境交互生成 S&#39; 和 R。&lt;/li&gt;
&lt;li&gt;Actor目标网络：负责根据经验回放池中采样的下一状态 S&#39; 选择最优下一动作 A&#39;，网络参数 $\theta&#39;$ 定期从 $\theta$ 中更新。&lt;/li&gt;
&lt;li&gt;Critic网络：负责价值网络参数 $w$ 的迭代更新，负责计算当前Q值 $Q(S,A,w)$。&lt;/li&gt;
&lt;li&gt;Critic目标网络：负责计算目标Q值 $y_i=R+\gamma Q&#39;(S&#39;,A&#39;,w&#39;)$ 中的 $Q&#39;(S&#39;,A&#39;,w&#39;)$ 部分，网络参数 $w&#39;$ 定期从  $w$ 更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ddpg算法流程&#34;&gt;DDPG算法流程&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;先初始化 Actor 和 Critic。然后把 Actor 和 Critic 的网络参数复制到目标 Actor 和目标 Critic 网络。&lt;/li&gt;
&lt;li&gt;将网络环境状态 S 输入 Actor网络得到动作 A，对环境施加动作 A，环境返回下一时刻的状态 S&#39; 以及奖励 R。并将这个四元组 $（S,A,R,S&#39;）$ 存入经验池。&lt;/li&gt;
&lt;li&gt;Critic 网络的更新。在经验池中积累一定数量的数据后，取出 mini-batch 大小的数据块，输入到 Critic 网络中，得到动作价值函数$Q(S,A,w)$。再将下一时刻状态 S&#39; 、动作 A&#39; 和上一时刻的奖励 R 一起输入目标 Critic 网络中，得到目标 Critic 网络的$Q_{target}=R+Q&#39;(S&#39;,A&#39;)$。Critic作为评委，一开始也不知道 Actor 输出的动作是否足够好，它也需要一步一步学习给出准确的打分，所以借助于目标网络拟合的下一时刻的价值 $Q&#39;$ ，以及真实的收益 $R$，我们可以得到 $Q_{target}$，让$Q_{target}$减去当前$Q$求均方差构造出Loss，再进行梯度下降来更新 Critic 网络参数。&lt;/li&gt;
&lt;li&gt;Actor 网络的更新。Actor的目标是找到最优动作 A 使得动作价值 Q 输出最大。因此，将通过将状态 S 以及 Actor 网络输出的动作 A 传入给 Critic 得到的结果$Q(S,A,w)$前面加负号作为损失函数，之后对其进行梯度下降进而实现 Actor 网络参数的更新。&lt;/li&gt;
&lt;li&gt;目标 Actor 网络和目标 Critic 网络的更新。我们每隔一段时间就把 Actor 网络和 Critic 网络的参数，拿去进行软更新目标 Actor网络和目标Critic 网络。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：两个Critic 网络的输入不同。Critic 网络中，动作和状态需要一起输入到 Critic 中；Critic 目标网络中，需要将动作、状态以及上一时刻的奖励一起输入Critic 目标网络。&lt;/p&gt;
&lt;img src=&#34;https://llc-typora.oss-cn-hangzhou.aliyuncs.com/typora/81be15e678ee87da6e19b066d0dcb34.jpg&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h1 id=&#34;ddpg关键技术&#34;&gt;DDPG关键技术&lt;/h1&gt;
&lt;p&gt;DDPG算法主要包括以下三个关键技术：&lt;/p&gt;
&lt;p&gt;（1）经验回放：智能体将得到的经验数据放入 Memory Pool 中，更新网络参数时按照批量采样。&lt;/p&gt;
&lt;p&gt;（2）目标网络：在 Actor 网络和 Critic 网络外再使用一套用于估计目标的Target Actor网络和Target Critic网络。在更新目标网络时，为了避免参数更新过快，采用软更新方式。&lt;/p&gt;
&lt;p&gt;（3）噪声探索：确定性策略输出的动作为确定性动作，缺乏对环境的探索。在训练阶段，给Actor网络输出的动作加入噪声，从而让智能体具备一定的探索能力。&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://famousli.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</category>
                                
                            
                        
                    
                        
                    
                
            </item>
        
    </channel>
</rss>
