<?xml version="1.0" encoding="utf-8"?>


<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN">
    <title type="text">李清华的博客</title>
    <subtitle type="html">李清华的博客</subtitle>
    <updated>2023-06-17T17:20:54&#43;08:00</updated>
    <id>https://famousli.cn/</id>
    <link rel="alternate" type="text/html" href="https://famousli.cn/" />
    <link rel="self" type="application/atom&#43;xml" href="https://famousli.cn/atom.xml" />
    <author>
            <name>李清华</name>
            <uri>https://famousli.cn/</uri>
            
                <email>famousli2799@gmail.com</email>
            </author>
    <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    <generator uri="https://gohugo.io/" version="0.113.0">Hugo</generator>
        <entry>
            <title type="text">DDPG算法</title>
            <link rel="alternate" type="text/html" href="https://famousli.cn/tech/drl-ddpg/" />
            <id>https://famousli.cn/tech/drl-ddpg/</id>
            <updated>2023-06-17T17:05:01&#43;08:00</updated>
            <published>2023-06-17T16:52:04&#43;08:00</published>
            <author>
                    <name>李清华</name>
                    <uri>https://famousli.cn/</uri>
                    <email>famousli2799@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">DDPG算法 DDPG算法一共包含了四个网络，分别是：Actor网络，Actor目标网……</summary>
            
                <content type="html">&lt;h1 id=&#34;ddpg算法&#34;&gt;DDPG算法&lt;/h1&gt;
&lt;p&gt;DDPG算法一共包含了四个网络，分别是：Actor网络，Actor目标网络，Critic网络，Critic目标网络，2个Actor 网络的结构相同，2个Critic网络的结构相同，四个网络的功能如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actor网络：负责策略网络参数 $\theta$ 的迭代更新，负责根据当前状态 S 选择当前动作 A，用于和环境交互生成 S&#39; 和 R。&lt;/li&gt;
&lt;li&gt;Actor目标网络：负责根据经验回放池中采样的下一状态 S&#39; 选择最优下一动作 A&#39;，网络参数 $\theta&#39;$ 定期从 $\theta$ 中更新。&lt;/li&gt;
&lt;li&gt;Critic网络：负责价值网络参数 $w$ 的迭代更新，负责计算当前Q值 $Q(S,A,w)$。&lt;/li&gt;
&lt;li&gt;Critic目标网络：负责计算目标Q值 $y_i=R+\gamma Q&#39;(S&#39;,A&#39;,w&#39;)$ 中的 $Q&#39;(S&#39;,A&#39;,w&#39;)$ 部分，网络参数 $w&#39;$ 定期从  $w$ 更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ddpg算法流程&#34;&gt;DDPG算法流程&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;先初始化 Actor 和 Critic。然后把 Actor 和 Critic 的网络参数复制到目标 Actor 和目标 Critic 网络。&lt;/li&gt;
&lt;li&gt;将网络环境状态 S 输入 Actor网络得到动作 A，对环境施加动作 A，环境返回下一时刻的状态 S&#39; 以及奖励 R。并将这个四元组 $（S,A,R,S&#39;）$ 存入经验池。&lt;/li&gt;
&lt;li&gt;Critic 网络的更新。在经验池中积累一定数量的数据后，取出 mini-batch 大小的数据块，输入到 Critic 网络中，得到动作价值函数$Q(S,A,w)$。再将下一时刻状态 S&#39; 、动作 A&#39; 和上一时刻的奖励 R 一起输入目标 Critic 网络中，得到目标 Critic 网络的$Q_{target}=R+Q&#39;(S&#39;,A&#39;)$。Critic作为评委，一开始也不知道 Actor 输出的动作是否足够好，它也需要一步一步学习给出准确的打分，所以借助于目标网络拟合的下一时刻的价值 $Q&#39;$ ，以及真实的收益 $R$，我们可以得到 $Q_{target}$，让$Q_{target}$减去当前$Q$求均方差构造出Loss，再进行梯度下降来更新 Critic 网络参数。&lt;/li&gt;
&lt;li&gt;Actor 网络的更新。Actor的目标是找到最优动作 A 使得动作价值 Q 输出最大。因此，将通过将状态 S 以及 Actor 网络输出的动作 A 传入给 Critic 得到的结果$Q(S,A,w)$前面加负号作为损失函数，之后对其进行梯度下降进而实现 Actor 网络参数的更新。&lt;/li&gt;
&lt;li&gt;目标 Actor 网络和目标 Critic 网络的更新。我们每隔一段时间就把 Actor 网络和 Critic 网络的参数，拿去进行软更新目标 Actor网络和目标Critic 网络。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：两个Critic 网络的输入不同。Critic 网络中，动作和状态需要一起输入到 Critic 中；Critic 目标网络中，需要将动作、状态以及上一时刻的奖励一起输入Critic 目标网络。&lt;/p&gt;
&lt;img src=&#34;https://llc-typora.oss-cn-hangzhou.aliyuncs.com/typora/81be15e678ee87da6e19b066d0dcb34.jpg&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;ddpg关键技术&#34;&gt;DDPG关键技术&lt;/h2&gt;
&lt;p&gt;DDPG算法主要包括以下三个关键技术：&lt;/p&gt;
&lt;p&gt;（1）经验回放：智能体将得到的经验数据放入 Memory Pool 中，更新网络参数时按照批量采样。&lt;/p&gt;
&lt;p&gt;（2）目标网络：在 Actor 网络和 Critic 网络外再使用一套用于估计目标的Target Actor网络和Target Critic网络。在更新目标网络时，为了避免参数更新过快，采用软更新方式。&lt;/p&gt;
&lt;p&gt;（3）噪声探索：确定性策略输出的动作为确定性动作，缺乏对环境的探索。在训练阶段，给Actor网络输出的动作加入噪声，从而让智能体具备一定的探索能力。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/398288352&#34;&gt;强化学习入门——深入理解DDPG&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/6869355403637964813&#34;&gt;强化学习13——Deep Deterministic Policy Gradient（DDPG）原理与实现&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_46133643/article/details/124356983&#34;&gt;深度强化学习-DDPG算法原理与代码&lt;/a&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
</feed>
